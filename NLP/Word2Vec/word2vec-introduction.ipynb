{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd \n",
    "\n",
    "# https://caelum-online-public.s3.amazonaws.com/1638-word-embedding/treino.csv\n",
    "artigo_treino = pd.read_csv('treino.csv')\n",
    "\n",
    "# https://caelum-online-public.s3.amazonaws.com/1638-word-embedding/teste.csv\n",
    "artigo_teste = pd.read_csv('teste.csv')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texto = ['tenha um bom dia', 'tenha um pessimo dia', 'tenha um otimo dia']\n",
    "\n",
    "vetorizador = CountVectorizer()\n",
    "vetorizador.fit(texto)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(vetorizador.vocabulary_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'tenha': 4, 'um': 5, 'bom': 0, 'dia': 1, 'pessimo': 3, 'otimo': 2}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "vetor_bom = vetorizador.transform([\"otimo\"])\n",
    "print(vetor_bom.toarray())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "with open('cbow_s300.txt') as f:\n",
    "    for linha in range(10):\n",
    "        print(next(f))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "929606 300\n",
      "\n",
      "</s> -0.001667 -0.000158 -0.000026 0.001300 -0.000796 0.001527 0.000046 0.000584 0.000449 -0.000100 0.000353 0.001251 0.001069 0.000506 0.000574 0.000838 -0.000930 -0.001220 0.000317 0.001315 -0.001120 0.001373 -0.000040 -0.001580 0.000421 -0.000667 -0.001556 -0.000746 0.001604 0.001157 -0.000027 0.000354 0.000358 -0.000527 -0.000573 -0.001512 -0.001557 -0.001637 0.001617 -0.001511 -0.001022 -0.001426 0.001086 -0.001033 0.000593 0.000724 0.000627 -0.000450 -0.001140 0.000333 0.000524 0.001541 0.000284 0.000617 -0.000807 -0.000088 -0.000364 0.001126 -0.001230 -0.001138 -0.001280 0.001330 0.001257 0.000576 0.000764 0.000684 0.001008 -0.000215 -0.000629 -0.001228 -0.001557 -0.000311 -0.000246 0.000045 0.001136 -0.000645 -0.000549 0.001099 0.000858 -0.000886 0.000553 0.000303 0.001433 0.000732 0.001321 -0.000894 -0.000700 -0.000661 -0.001484 -0.000950 -0.001556 -0.000809 0.000348 -0.000068 0.000724 -0.000569 -0.000161 -0.001628 -0.001437 -0.000259 -0.000296 -0.001571 0.000149 0.000847 0.000613 0.000802 0.001507 0.001015 0.000377 0.000255 -0.000458 -0.000777 -0.001561 0.001601 -0.001520 -0.001210 0.000106 0.000714 0.000392 0.001311 -0.001192 -0.000090 -0.001097 0.000424 -0.000954 -0.001272 -0.001178 0.000036 -0.000181 0.000331 -0.001453 -0.001488 -0.001033 -0.000377 0.000257 -0.001418 0.001109 0.000722 0.000936 -0.000113 0.001215 -0.000263 0.000652 0.001190 -0.000258 0.001391 0.001213 0.000783 -0.001202 0.000470 -0.000879 0.000688 -0.001163 -0.001105 0.001497 0.001304 -0.001322 -0.001501 0.001377 0.001439 0.000884 0.000484 0.001239 -0.001578 0.000981 -0.000318 -0.001180 -0.001375 -0.001491 0.001057 -0.001028 0.000893 0.001028 0.000772 0.001636 -0.000331 -0.000247 -0.001006 -0.000329 0.000837 0.000605 -0.000959 0.001410 0.000488 0.001167 -0.000293 -0.001188 -0.000001 0.001135 0.001141 0.001504 0.000198 -0.001060 0.001551 -0.000003 -0.001474 -0.000391 -0.000880 0.000433 -0.000976 -0.001417 0.000563 -0.001188 0.000593 0.001584 -0.001602 -0.000439 -0.001148 -0.001256 0.001185 -0.000738 0.001543 -0.000846 -0.001029 -0.000641 -0.001587 0.001439 -0.001251 0.000942 -0.001414 -0.001106 0.001087 -0.000027 0.000757 -0.000159 -0.001014 -0.000891 0.000024 -0.000238 0.000157 -0.001067 0.000902 -0.001050 -0.000428 -0.001606 -0.000988 0.001391 0.001165 -0.000113 -0.001000 -0.000055 -0.001369 0.000684 0.000715 0.001407 0.000613 0.001389 0.001315 -0.000130 -0.001044 0.000175 -0.000035 0.000959 -0.000345 0.001209 -0.001251 -0.001219 0.001231 -0.000996 -0.001388 0.001038 0.001336 -0.001066 -0.000881 -0.001066 -0.001466 -0.000274 0.000201 0.000401 0.000132 0.000588 0.000589 -0.000128 0.001073 0.001197 0.000109 0.000770 0.001221 0.000996 -0.001174 0.000135 -0.001134 -0.001385 -0.000311 -0.001631 -0.000564 0.001162 -0.000322 -0.000469 0.001312 -0.001402 0.000239 0.000184 0.001300 0.000021 -0.001065 0.000047 -0.000301 0.001336 0.000332\n",
      "\n",
      ", -0.061483 -0.094368 -0.008557 -0.034702 0.021108 -0.011873 -0.041133 -0.095925 0.034668 -0.085286 0.076174 0.003314 0.019222 -0.038695 -0.008963 0.053399 0.162935 0.050372 -0.020163 -0.027230 0.061531 0.060840 0.074610 -0.056173 0.007621 -0.055220 0.018008 0.026096 0.033154 -0.082612 -0.081761 0.164978 -0.034423 0.003094 -0.018217 -0.087445 -0.074446 -0.000142 0.004218 0.036585 0.016095 -0.129141 -0.119698 -0.053717 0.005053 -0.114520 -0.017219 0.023693 -0.024115 0.053125 0.024658 -0.037689 0.012078 0.112701 0.028037 0.047618 -0.024196 0.050112 -0.073095 -0.090859 -0.030613 -0.109599 0.037756 0.063827 0.022537 -0.029640 -0.016311 0.021875 0.064882 0.039002 -0.082970 -0.043166 0.013695 -0.043153 -0.082203 -0.020087 -0.100360 0.007033 -0.074208 -0.067789 -0.024897 -0.020358 0.041731 0.101332 0.020217 -0.032473 0.087827 -0.033611 -0.150526 -0.016615 0.021147 -0.025058 0.097833 0.065067 0.051287 -0.079191 0.089563 -0.008436 -0.038352 0.019787 -0.058452 -0.009696 -0.051077 -0.112245 0.024886 -0.015172 -0.129670 0.068672 0.068483 0.009049 0.007055 -0.032763 0.001527 0.054264 0.029924 -0.023482 0.047470 0.008044 0.014534 0.071155 0.016700 -0.027491 -0.155782 0.039370 0.116605 -0.001262 -0.026638 -0.067078 0.078015 -0.066153 -0.039303 0.009535 -0.055698 -0.022250 0.046948 0.053810 0.038096 0.032157 -0.075257 0.008125 -0.034598 -0.020667 -0.003153 0.032491 -0.031064 0.030744 -0.049023 -0.046149 0.000792 0.010385 -0.057119 -0.122554 0.003210 -0.054363 -0.100899 0.069873 -0.009758 0.055455 0.049243 0.008346 -0.016087 0.093572 0.024125 0.058736 0.037243 -0.007478 0.032175 -0.054205 0.008798 0.032326 0.028384 -0.032259 -0.041842 -0.058281 -0.025145 0.011097 0.023598 -0.033376 0.026204 0.032505 -0.009283 0.041076 0.055565 -0.081757 -0.010077 0.058251 -0.014379 -0.040951 0.006938 -0.004179 0.052006 -0.063725 0.035674 -0.066554 0.030910 -0.004032 0.077445 0.029495 -0.064931 0.040263 -0.055423 -0.021571 0.086767 -0.003583 0.073308 -0.043991 0.022503 -0.028692 -0.063626 -0.048238 0.013439 -0.043673 -0.101352 -0.004321 0.125507 0.088486 0.042756 -0.014497 -0.053445 0.021800 0.038406 -0.034023 -0.074428 -0.132825 0.082152 -0.068497 0.004738 0.047527 -0.073890 0.051089 -0.055886 -0.047786 0.040247 -0.053966 -0.015752 0.099451 0.008218 -0.010716 -0.031540 0.036168 0.054244 0.051809 0.035158 0.043006 -0.027902 0.000130 0.103397 -0.114831 -0.036648 -0.036143 0.024432 0.084740 0.001801 0.044475 -0.035746 -0.024109 0.051210 -0.025769 0.016073 -0.000351 -0.029183 -0.075292 0.042163 0.025010 -0.041439 -0.059192 0.026617 -0.040852 0.034697 0.014691 -0.057382 0.046141 0.070360 0.045274 0.065880 0.011023 -0.031292 -0.015784 -0.023421 -0.042788 0.019669 0.035010 0.036188 -0.058060 -0.093562 0.030321 -0.054753 0.097162 0.001134 0.018939 -0.150218 -0.009928 0.051118 0.105212 -0.055051 -0.047959 -0.136800 -0.003198 0.068969 -0.022456\n",
      "\n",
      "de -0.232068 0.066729 0.103946 -0.072608 0.126237 -0.004782 -0.025139 -0.141489 -0.069438 -0.071078 0.175772 -0.017257 0.094824 0.011020 0.029226 -0.010670 0.144973 0.105333 -0.088273 -0.070952 0.054747 -0.048955 -0.047809 -0.030763 -0.052293 -0.003596 0.078465 0.144430 0.129697 -0.078427 -0.025080 0.212887 0.119806 0.101703 -0.142488 -0.031272 -0.026594 -0.109429 -0.154688 -0.121492 -0.103781 -0.177948 -0.125544 -0.120136 0.031658 0.054160 0.060493 -0.115676 0.132511 -0.001668 -0.125569 -0.124120 0.029025 0.006200 -0.007915 0.058489 0.122710 0.004660 -0.082200 0.123740 0.052747 -0.065657 0.154747 0.155071 0.003547 -0.154675 -0.035110 0.119117 0.080579 0.044262 0.048753 0.054975 -0.064624 -0.046866 0.028242 0.144197 0.029916 -0.024569 0.195781 -0.028608 -0.008233 -0.032542 0.012042 0.011934 -0.068173 0.037028 0.018363 -0.038244 -0.091176 -0.019795 0.051640 -0.008025 -0.025825 -0.001686 -0.069845 -0.039987 0.066244 0.088229 -0.171358 -0.030783 0.082344 0.059267 0.000550 -0.216285 -0.201356 -0.131063 -0.033069 -0.057126 -0.127576 -0.099251 -0.044669 0.035363 -0.119503 -0.090019 -0.006226 -0.000546 0.020731 0.016140 -0.007438 0.032885 -0.161048 0.105681 -0.115705 -0.078013 0.102293 -0.107958 -0.025926 -0.067986 0.017502 -0.054865 -0.007930 -0.006999 -0.000949 -0.026097 0.106295 0.115368 -0.033742 -0.046025 0.009641 -0.065890 0.198822 0.078333 0.062547 0.044947 -0.098424 0.140951 0.109045 -0.004204 -0.174950 0.034234 0.040793 -0.210289 0.104861 0.083616 -0.042511 -0.031439 -0.113108 0.000401 -0.171665 0.097890 0.008065 0.080684 -0.009576 -0.042125 0.177337 -0.005585 -0.004165 0.069755 -0.100323 -0.023289 0.039458 -0.089292 0.069543 -0.009576 0.011562 -0.076654 0.009681 -0.018808 -0.047881 0.026709 -0.021200 -0.032544 0.100362 0.037157 0.005169 -0.001280 -0.064330 -0.049024 -0.002354 0.004443 0.049129 0.026813 0.034249 0.068827 -0.038583 -0.116914 -0.107378 0.046983 0.038218 -0.082186 -0.124208 0.066872 -0.081745 -0.016516 0.016110 0.046844 0.176223 0.083604 -0.086893 -0.114739 -0.159588 0.007700 0.004887 0.006024 -0.020026 0.045816 0.033604 0.054474 0.089348 -0.024353 0.104835 0.084334 0.052662 -0.041422 -0.027877 0.002816 0.150068 -0.052310 0.017154 -0.152326 0.067753 -0.082644 0.119430 -0.012345 0.082965 -0.005791 -0.082770 -0.030068 -0.037331 -0.075347 0.035060 -0.092023 -0.001051 0.012675 0.128757 -0.048579 -0.078527 -0.134126 -0.060009 0.096834 -0.045947 0.132404 -0.030576 -0.006618 -0.088179 -0.124597 -0.095311 -0.086943 0.007010 0.059925 -0.077005 -0.035458 -0.017957 0.081104 0.060141 0.152996 0.083737 -0.025909 0.005420 -0.006300 -0.075839 -0.012399 -0.001624 0.039090 -0.040755 -0.013051 -0.072733 -0.048062 -0.082573 -0.013851 -0.120222 0.011452 -0.083538 0.015996 -0.110800 0.012405 0.045823 0.026705 -0.040789 0.064309 0.007381 -0.037854 0.076050 0.104702 0.010307 -0.103478 0.085227 0.064233 -0.015908 -0.047998\n",
      "\n",
      ". 0.027867 0.077901 -0.054738 -0.095938 -0.010536 0.015269 -0.005752 -0.048440 -0.104021 0.054583 0.050108 0.054979 0.071112 -0.056665 -0.140868 0.203626 0.211330 0.137270 -0.038069 -0.108607 0.060027 0.052436 0.126758 -0.078212 0.008864 -0.028087 0.029884 0.114899 -0.058657 -0.166441 -0.188866 0.294844 -0.147703 -0.095883 -0.104021 -0.097608 -0.172317 0.035795 -0.019025 0.057503 -0.030564 -0.125182 -0.257359 -0.106566 -0.002708 -0.035769 -0.072053 -0.005640 0.016920 0.155101 -0.226088 -0.070904 -0.110280 -0.075773 -0.190123 0.197397 -0.108603 0.155954 0.096544 -0.183301 0.008154 -0.195997 0.035284 -0.025561 0.159142 -0.141005 0.140643 -0.160588 0.140709 0.059974 -0.012396 -0.074953 -0.038325 -0.165941 -0.226005 -0.043738 0.052905 0.085072 -0.165731 0.016894 -0.136569 0.088165 -0.032044 0.022789 0.051207 0.015907 0.093662 0.037576 -0.286139 -0.107587 0.165001 0.095293 0.181427 0.038072 0.078180 -0.116599 0.138464 0.138386 0.020201 -0.028087 0.038829 -0.040074 -0.044944 -0.179895 0.035044 0.090347 -0.036088 0.154828 0.090421 0.172175 -0.127294 0.060227 0.164694 0.141367 0.049326 -0.136940 0.060633 -0.089581 -0.069639 0.043680 0.141455 -0.060588 -0.118859 0.181506 -0.051302 0.165763 0.151366 -0.191232 -0.103888 -0.077344 -0.016534 0.073607 -0.008464 0.216368 0.038251 0.002552 -0.112613 -0.049596 0.018040 -0.032907 -0.131335 -0.049530 0.057485 0.129446 -0.017884 0.075667 -0.061577 -0.015305 0.007649 0.116636 -0.132568 -0.244250 -0.048748 0.003139 -0.123311 0.018462 0.126955 0.079116 -0.067653 0.019126 0.044182 0.037610 0.199809 -0.118487 0.011276 -0.061652 -0.034426 -0.120148 -0.171490 0.009182 0.182921 -0.165212 0.149898 0.062080 -0.129040 0.051808 -0.075070 -0.068688 0.018707 -0.008050 0.108429 -0.111089 0.021230 -0.125982 0.077740 0.140251 -0.051900 0.036107 0.019580 -0.084050 0.065747 0.015718 -0.016900 -0.026964 0.126565 -0.087491 0.069504 0.108362 -0.006355 0.118521 -0.002510 0.060457 0.082848 0.000294 -0.037938 -0.026097 0.084446 0.015568 -0.050540 -0.117080 0.100227 -0.046050 -0.059615 0.086632 0.097266 0.084578 -0.036891 -0.033246 0.021302 -0.149581 -0.006353 -0.035713 -0.177163 -0.016629 -0.028634 -0.147015 -0.017861 0.009931 -0.181849 0.065564 -0.007616 0.094861 0.089787 0.005743 -0.127185 0.093027 -0.107247 -0.019161 -0.201082 -0.008504 0.126500 0.034318 0.011876 0.012696 -0.090981 0.012814 0.038388 -0.274763 -0.123303 -0.073078 0.027702 0.035841 -0.080954 0.091305 0.029009 -0.006000 0.121087 0.016678 -0.070181 -0.080839 -0.016332 -0.081390 -0.059594 0.131390 0.039603 -0.222503 -0.039382 -0.071041 0.267399 -0.010011 -0.077326 -0.044107 0.052901 0.023623 -0.087243 0.011539 -0.006381 -0.074194 -0.041591 -0.061872 -0.098552 -0.094357 -0.062879 -0.150341 -0.087596 0.330078 0.028435 0.087784 0.034434 -0.008122 -0.258485 -0.109828 0.013279 0.043340 0.136803 -0.081573 -0.136733 -0.081884 0.306241 0.093106\n",
      "\n",
      "a -0.019764 -0.096043 -0.010960 0.102012 -0.101848 -0.010257 0.004692 -0.165735 -0.179723 0.067659 0.037108 0.100931 -0.211779 0.163603 -0.042376 -0.021683 -0.014900 0.101581 -0.053583 0.065435 0.126251 0.062320 -0.057445 -0.107130 -0.037044 -0.072958 -0.066655 -0.067962 -0.070879 -0.187941 0.065208 0.102302 -0.024876 0.151433 0.012228 -0.065570 -0.063793 0.015519 0.046230 0.174824 -0.100376 -0.081244 -0.039337 0.017851 0.125168 -0.096404 -0.133070 -0.023451 0.117965 0.005949 0.038928 0.060488 -0.014678 0.024891 -0.001455 0.048539 0.133535 0.084564 0.018362 -0.054294 -0.235691 -0.151528 0.043101 -0.097685 -0.034775 -0.127507 0.010346 -0.010458 0.047631 0.002045 -0.013826 -0.139644 0.128220 -0.020997 -0.057176 0.007458 -0.033133 -0.083235 -0.075585 -0.079414 -0.023531 0.076794 0.051968 0.026857 0.007574 -0.039602 0.027458 -0.146083 0.008662 0.113647 0.007963 0.023413 0.088046 -0.079530 0.027995 -0.017113 -0.056750 0.069447 0.094213 0.123728 0.010051 -0.054795 0.041003 -0.044294 -0.066911 -0.025221 0.059618 0.023494 -0.095354 0.029266 0.047979 0.079102 -0.061757 0.116363 0.011112 0.136784 -0.027020 -0.002793 -0.092629 -0.047217 0.111263 0.102819 0.090932 -0.085343 -0.026490 -0.033315 -0.028970 -0.118673 -0.067360 -0.207183 -0.076011 0.017469 -0.078908 0.031023 0.090820 -0.054443 0.062466 0.065952 -0.028265 0.051329 0.090671 0.106923 0.028011 0.014720 -0.036025 0.099207 -0.013510 -0.147551 -0.077657 -0.099269 0.001171 0.059405 -0.006946 0.123997 0.023131 0.102308 -0.040784 0.027455 -0.181898 0.095958 0.155467 -0.077250 -0.124941 0.058529 0.035851 -0.004447 0.035127 -0.024015 -0.029101 -0.107558 0.055650 -0.017027 0.008997 -0.016504 0.018768 -0.080671 0.029003 0.013937 0.002876 -0.064262 0.001246 0.073494 -0.013986 -0.100303 0.027376 0.030375 -0.177618 0.016413 -0.008711 -0.257308 -0.031904 0.068326 0.038748 0.006762 -0.045245 0.038158 0.093055 -0.069432 -0.140442 -0.030486 -0.097529 0.251017 -0.059000 -0.037363 -0.020030 0.116937 0.018191 -0.005464 0.016485 0.009930 0.007318 -0.131821 -0.005316 -0.063905 0.066936 -0.007235 0.066709 0.153262 -0.087015 -0.196866 0.042973 0.124447 0.034619 -0.021113 0.029881 -0.072679 0.068707 -0.033823 -0.140311 -0.158137 0.215081 -0.020170 -0.062018 -0.038955 0.158768 -0.155656 0.016471 -0.071324 0.069629 -0.070692 0.114725 0.114054 0.154133 -0.143244 -0.082015 -0.070578 0.094558 -0.073491 -0.084676 0.089027 -0.053428 0.034783 0.007488 0.008849 0.137206 -0.061942 -0.155206 -0.022297 -0.054165 0.206418 0.058566 0.033847 0.039555 0.042072 -0.003160 0.350700 -0.067416 -0.089351 0.147734 0.026404 -0.128322 -0.143885 -0.189820 0.162824 -0.038651 -0.045660 0.083507 0.118510 -0.054740 -0.053446 -0.106545 -0.087144 -0.159905 0.006153 0.057539 0.079486 -0.015659 -0.014220 -0.013549 -0.122177 -0.154875 0.319072 -0.251233 0.070468 -0.084041 -0.067580 -0.163386 -0.026553 -0.033245 -0.035199\n",
      "\n",
      "o 0.050016 0.094213 -0.234393 0.057870 -0.255710 -0.099076 0.162541 -0.073166 -0.072231 0.087253 0.154162 -0.021438 0.095246 0.086680 0.027481 0.055959 0.176635 -0.158901 -0.076106 -0.036695 0.115689 -0.039255 -0.116532 -0.143528 0.059030 0.049429 -0.009721 0.204578 -0.020946 -0.254149 -0.167956 0.024087 -0.094412 -0.066287 -0.085987 -0.081115 -0.035314 -0.000736 0.069831 -0.034319 -0.041741 0.007945 0.106777 0.119971 -0.051358 0.152951 -0.109669 0.102599 0.102598 0.044424 -0.088313 0.142374 0.042868 -0.042467 0.199348 -0.234265 -0.044845 -0.109492 0.005783 -0.045873 -0.049017 -0.037446 0.105329 -0.163747 -0.139140 0.153179 -0.117726 -0.113436 0.010813 0.097110 0.074618 0.041083 0.082912 -0.052041 0.009895 0.160828 0.058190 -0.215907 -0.024543 0.042466 -0.011712 0.064930 0.097498 -0.080108 0.019837 -0.059179 0.077248 -0.135312 0.132748 0.069083 0.075949 -0.232743 0.078279 -0.078944 0.116486 0.004099 0.057786 -0.125938 0.068976 0.176170 0.044454 0.085901 0.033849 0.057793 -0.083662 0.061258 -0.037814 0.050392 -0.067302 -0.102103 -0.011153 0.029286 0.032976 0.003400 0.061664 0.076337 -0.097293 0.026432 -0.059397 -0.160978 0.069399 0.307550 0.069981 -0.122410 0.003086 0.027841 0.134213 -0.018279 -0.025608 -0.156602 0.062460 -0.051757 0.030337 -0.042270 0.007516 0.055478 0.075345 0.112685 -0.047181 0.001009 0.031433 -0.134699 -0.034702 -0.082685 -0.182748 -0.097813 0.053774 -0.067876 -0.090774 -0.035145 0.080364 -0.242563 0.024905 -0.012293 0.110051 0.103592 0.138403 -0.035121 0.037955 -0.028042 0.005687 0.042462 0.045434 0.127590 -0.049950 0.079609 -0.152268 -0.102837 -0.083840 -0.132681 -0.073176 0.031672 0.104991 0.082840 0.142188 -0.047643 0.054661 -0.112325 0.006551 0.079335 0.011705 -0.042121 0.086597 0.088994 0.079949 -0.002677 0.106816 -0.055035 0.068153 -0.167192 0.009149 -0.028594 -0.039560 0.001607 -0.122615 0.106024 0.107250 0.050540 -0.043291 0.061397 0.001380 0.054964 0.040893 -0.045046 0.002312 0.032035 -0.026698 -0.042064 0.144622 0.076241 0.024255 -0.086833 0.098275 -0.083894 0.086363 -0.004560 -0.029056 0.126784 0.071785 -0.025692 0.000966 0.100620 0.030299 -0.163968 0.042070 -0.198341 0.099603 -0.113758 0.124685 -0.069966 -0.165131 -0.134782 -0.020419 -0.112747 0.123515 -0.045274 0.167949 -0.109681 -0.146224 0.075453 0.054352 0.023153 -0.062161 -0.059457 -0.124327 0.041855 0.205124 -0.083616 -0.062827 0.017066 0.025000 0.079311 0.018874 -0.055492 -0.129246 0.099183 -0.022749 -0.079510 -0.130005 0.012160 0.060842 0.018816 -0.180103 0.057191 0.018971 0.216519 -0.003726 0.155861 0.112271 0.091379 -0.073707 -0.059254 0.011068 -0.005690 -0.098677 -0.082454 0.105838 0.042144 0.100894 0.096960 -0.228625 0.064737 0.022971 -0.001549 -0.105277 0.149519 -0.055187 0.201671 0.098550 0.056229 -0.107189 -0.005223 -0.077126 0.020865 -0.098107 0.064205 -0.174567 0.196321 0.233813 0.038634\n",
      "\n",
      "e -0.168088 -0.001531 0.017038 -0.137315 -0.026399 -0.018215 0.026076 0.019779 -0.095212 -0.090352 -0.106060 0.061975 -0.000318 -0.002945 0.119734 0.149804 0.108117 0.064784 0.065574 -0.038343 0.159915 -0.006124 -0.038307 -0.094810 -0.019356 -0.045115 -0.121859 0.028930 0.045017 -0.138268 0.099283 0.128659 0.077656 0.032982 -0.086139 0.051552 -0.077685 -0.066857 -0.045233 -0.185625 0.013728 -0.067088 -0.061796 0.091931 0.007931 -0.033784 -0.026604 0.026487 0.002281 0.000939 -0.043053 -0.082718 -0.101476 0.014253 0.106507 0.059285 -0.034097 0.032848 -0.067852 0.103892 0.020742 -0.049702 0.085734 0.069140 -0.036293 -0.001048 -0.043403 -0.033758 -0.041652 -0.049051 -0.070893 -0.002921 0.096254 -0.046406 -0.016074 0.074894 0.098854 -0.015867 -0.024065 0.048818 -0.058226 -0.031364 0.055631 -0.001012 0.002966 0.017624 0.082472 -0.025849 -0.099736 -0.065253 0.002511 0.026550 -0.046750 0.005665 0.058496 -0.068569 0.110588 -0.003354 -0.133236 -0.140404 0.090284 -0.089065 0.010600 -0.143848 0.023044 0.035726 0.044885 -0.081761 0.020406 0.045142 -0.099682 0.013478 -0.102208 -0.048206 0.040736 0.012733 0.064284 0.052800 0.046179 -0.083605 -0.002064 0.003024 0.078588 -0.075731 -0.038192 -0.010662 -0.062210 -0.060302 0.005740 -0.119290 0.049789 -0.009878 0.033791 0.145393 -0.064118 0.005292 -0.046015 -0.012572 -0.002266 0.018877 -0.114718 0.043831 0.036031 -0.077071 0.079499 0.047130 0.136516 -0.029402 -0.095073 -0.198367 0.077792 -0.092858 -0.115212 0.064883 -0.083748 -0.115535 -0.017903 0.080732 0.088457 0.037072 0.044588 0.042071 0.064027 0.035189 0.071027 -0.028330 0.026847 -0.005846 -0.003655 -0.019447 0.057641 -0.057623 0.032575 -0.009248 0.103035 -0.087592 0.049022 -0.030181 0.055313 0.001550 0.136222 -0.081518 -0.003322 -0.196934 -0.125240 -0.039119 0.082201 -0.036932 -0.030666 0.101376 -0.008224 0.045433 0.055995 -0.108370 0.120019 0.024427 -0.055956 0.076503 -0.063588 -0.091926 -0.047620 0.071838 -0.050062 0.122101 0.011679 0.015549 0.060463 -0.174818 0.006131 -0.035602 -0.039755 0.044683 -0.002324 -0.096353 0.153145 -0.181759 0.059133 0.008242 0.009471 0.004472 -0.005789 0.035746 0.107733 0.019606 0.035323 0.013101 0.078772 0.031495 0.011914 0.035316 0.029965 0.010626 0.024444 -0.036373 0.041982 -0.059195 -0.062521 -0.032445 -0.050086 -0.002740 -0.033702 -0.052157 -0.100889 0.010753 -0.017195 0.076872 -0.114096 -0.134334 0.019812 -0.147168 -0.048026 0.055315 -0.021564 0.092953 -0.070098 -0.009133 -0.021408 -0.043442 -0.059603 -0.026196 -0.046402 -0.024446 0.036482 -0.021195 -0.014285 -0.003113 -0.053620 -0.057722 -0.005912 -0.228597 -0.022520 0.008453 -0.131441 0.074053 0.010990 -0.029988 -0.106588 -0.005512 0.036736 -0.002115 -0.078824 -0.067073 0.034071 -0.037642 0.019106 0.080103 0.068267 -0.052487 -0.070635 0.073949 -0.056421 0.017687 0.011800 -0.014045 0.006226 -0.014210 0.088889 -0.007613 -0.042151 0.091935\n",
      "\n",
      "que 0.105677 -0.012054 -0.134709 -0.013888 -0.080373 -0.084105 0.028820 -0.039461 -0.011410 0.033408 -0.031719 0.198935 0.047353 -0.090613 -0.098891 -0.010202 0.144315 0.151674 0.016502 -0.133896 0.031100 -0.007432 -0.059600 0.059022 -0.038658 0.167877 0.082268 0.217249 -0.072088 0.076910 0.079216 -0.037108 0.014821 -0.078732 -0.035230 0.024507 -0.126809 -0.017172 0.040998 -0.043692 -0.149663 -0.049520 0.024513 -0.063125 0.058306 -0.012108 -0.119330 0.034781 -0.102405 0.088640 -0.272843 -0.059915 -0.108119 -0.004685 0.184494 0.029357 -0.012836 0.087409 -0.093633 0.141491 -0.051050 -0.072004 -0.164353 -0.078488 -0.075351 -0.029836 -0.011346 -0.081588 0.032113 -0.120232 -0.028536 0.010188 0.169394 -0.077409 -0.026955 0.083380 0.010644 0.026978 0.131446 -0.055521 -0.142428 -0.164762 0.016372 -0.254786 -0.015511 -0.044090 -0.091434 0.042059 -0.182369 -0.057621 -0.113482 -0.031757 0.036860 0.035760 -0.084302 -0.244284 0.014317 -0.110019 0.076959 -0.003242 0.058425 -0.068168 -0.150191 -0.054872 0.067636 -0.130009 0.033544 -0.028672 -0.029417 -0.003639 0.008028 0.067533 0.015459 0.057868 -0.095344 0.023596 0.005216 -0.068982 0.110498 -0.045724 0.067068 -0.103614 -0.010749 0.079515 -0.160577 -0.037640 -0.090058 0.194754 -0.137685 0.048330 0.023402 -0.080694 -0.076719 0.081176 0.145217 -0.312484 -0.158713 0.003970 0.017241 0.020559 0.019566 -0.020686 0.027909 -0.088033 0.055311 0.114986 0.046477 -0.026289 -0.085503 0.072325 0.033167 -0.027302 0.043202 -0.125175 -0.046430 -0.089749 -0.052517 -0.026923 -0.157140 0.142859 -0.035472 0.053636 -0.015367 -0.041692 0.004743 -0.053162 -0.073490 -0.031711 0.009967 -0.034655 0.209237 -0.052801 0.121190 -0.048079 0.203458 0.037113 -0.095015 0.143972 0.046959 -0.014909 0.169301 -0.034984 0.006088 -0.180444 -0.098740 -0.082224 0.042493 -0.048264 0.034078 -0.155237 0.049812 0.108571 -0.103458 0.015055 -0.005921 -0.020380 -0.059664 0.063496 0.120677 0.028720 0.010933 -0.034094 -0.033195 0.065065 -0.150067 -0.127015 -0.115948 -0.168308 -0.079605 0.112220 -0.024935 0.008233 -0.050985 -0.047235 -0.041944 0.099358 -0.094031 0.008722 -0.044243 0.091927 0.029639 0.053425 -0.020329 -0.007264 0.029055 -0.027748 0.021123 0.023926 -0.068259 -0.034626 0.010976 -0.027882 0.074465 0.017960 -0.142224 0.010472 -0.005171 0.001003 0.031482 0.048073 0.122005 -0.012546 -0.127375 0.137744 -0.043389 -0.067031 -0.028339 -0.102349 -0.001278 -0.075125 -0.209901 0.022713 0.123317 -0.005108 0.206613 -0.119748 0.148568 0.189863 -0.005551 0.042991 0.024824 0.095960 0.049202 -0.115647 -0.075263 -0.101255 0.047671 0.006936 -0.147872 -0.048649 0.152138 0.090100 -0.054797 0.180955 -0.016752 -0.090723 -0.126502 0.137744 0.065590 -0.095213 -0.141592 -0.081258 0.061414 0.138765 -0.047507 0.155577 -0.004058 -0.054305 -0.064755 -0.155780 0.019290 0.092772 -0.113740 0.062508 -0.108115 -0.104516 0.233178 0.085033 -0.063993 0.107476\n",
      "\n",
      "do -0.180377 0.215497 -0.243985 -0.111583 -0.213779 -0.101028 -0.000539 -0.101256 -0.070914 0.038753 0.046168 -0.014838 0.180611 0.000883 0.060867 0.085324 0.226791 -0.021134 -0.137997 -0.202197 0.148571 -0.050565 -0.078147 -0.128638 -0.060884 0.050100 0.018876 0.214215 0.115665 -0.095654 -0.142162 0.120025 -0.097131 0.072397 -0.038019 0.105100 0.071983 -0.160280 -0.051489 -0.098811 -0.117858 -0.034939 -0.009614 0.107021 -0.047199 -0.005264 -0.000139 0.071706 0.161239 -0.161907 -0.337947 0.006058 0.022506 -0.037415 0.171261 -0.062566 -0.062829 -0.086872 0.049601 0.208633 -0.010851 0.096788 0.242130 -0.103836 -0.078473 -0.008937 -0.192728 0.135349 -0.090459 0.275743 0.134895 0.051969 -0.132400 0.040473 -0.095963 0.027610 0.029590 -0.188700 0.164033 0.032661 -0.109406 0.180492 0.147311 0.025806 -0.079043 -0.050046 0.065651 -0.066955 0.037583 -0.031008 0.012122 -0.225802 0.106807 0.067123 0.035571 -0.204494 -0.004813 -0.022719 -0.267100 0.241056 0.115854 0.094971 -0.032575 0.045279 0.030343 -0.103868 0.013372 0.008258 0.027818 -0.051606 0.060680 0.094947 -0.025672 -0.044170 -0.007713 -0.008499 -0.111320 0.112186 -0.091970 -0.085167 0.001894 0.156088 -0.067075 -0.027825 0.093561 0.070251 0.049796 -0.161722 -0.051384 -0.124694 -0.016951 -0.141747 0.002894 -0.133038 -0.015664 0.080839 -0.090980 0.084050 0.089413 0.021215 0.043560 -0.075679 -0.068300 -0.025250 -0.157061 0.022866 0.055717 -0.019705 0.036159 0.183264 0.073571 -0.227341 -0.020978 0.064482 0.075304 0.010691 -0.068159 0.110613 0.047416 -0.029155 -0.043249 -0.074944 0.011065 -0.047526 0.018594 0.114240 -0.265600 0.007173 -0.118795 -0.109128 0.000016 -0.025266 -0.101829 0.167882 0.163892 -0.112688 -0.195755 -0.162868 0.114179 0.149630 0.075342 0.024940 0.099575 0.154626 0.069327 0.046685 0.046306 -0.138379 0.070196 -0.071198 0.028745 0.052231 -0.034552 -0.065006 0.070413 -0.061854 0.102099 0.063467 -0.002194 0.041076 -0.095035 0.138216 0.123084 -0.022177 0.178009 0.171209 0.193143 -0.170747 0.013212 0.020312 -0.129422 0.079598 -0.068529 -0.147807 0.047131 -0.137531 0.091482 0.030125 0.145402 -0.080191 0.025542 0.178609 0.118927 -0.206678 0.094064 -0.042739 -0.084631 -0.076197 0.253984 -0.273463 -0.026424 0.022400 0.012012 0.061563 0.131845 0.000155 -0.068048 -0.138741 -0.137770 0.143289 -0.017924 -0.111125 -0.055598 -0.012842 0.040329 0.034049 0.148260 -0.121714 -0.091318 0.030290 0.127014 0.037068 -0.070445 -0.136299 -0.232076 -0.028718 0.111683 -0.116630 -0.077730 0.033543 -0.032343 0.091454 0.094135 0.163275 0.123356 0.155053 0.137251 0.054886 0.086155 0.132626 -0.112011 0.103465 -0.085405 -0.075092 0.038307 0.075783 0.028715 0.020250 0.078956 0.012173 -0.058316 0.084967 -0.099896 -0.090466 -0.126150 0.144591 0.025099 0.059846 0.102206 0.092871 -0.004201 -0.062104 0.143311 -0.046741 0.010705 -0.065122 -0.074470 0.072302 0.144289 0.067299\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# http://www.nilc.icmc.usp.br/embeddings\n",
    "model = KeyedVectors.load_word2vec_format('cbow_s300.txt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/pedro/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(model.get_vector('china'))\n",
    "print(len(model.get_vector('china')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-1.49033e-01  1.26020e-01  2.17628e-01  1.82684e-01  1.65151e-01\n",
      " -1.59660e-01 -2.34411e-01  6.00570e-02  8.03680e-02  2.87578e-01\n",
      " -4.81100e-03 -5.68800e-02  2.15676e-01  8.65540e-02  1.25983e-01\n",
      "  3.36157e-01 -1.83254e-01 -1.18499e-01  1.13010e-02  1.03814e-01\n",
      "  9.37640e-02  2.90178e-01 -1.64395e-01 -1.13300e-02 -1.80676e-01\n",
      " -1.15820e-02  1.08728e-01  1.65898e-01  9.37900e-02  2.66767e-01\n",
      " -1.29890e-02  9.16030e-02  2.21292e-01 -1.36497e-01 -4.26350e-02\n",
      " -1.30038e-01  2.17067e-01 -1.01963e-01 -3.70960e-02  1.42155e-01\n",
      "  3.41109e-01  2.46560e-01  1.27458e-01  5.72360e-02 -1.47962e-01\n",
      " -1.60290e-02  1.86533e-01  7.71550e-02 -3.50024e-01 -4.06085e-01\n",
      "  1.67131e-01 -4.75230e-02  5.13780e-02 -1.28224e-01  1.06580e-02\n",
      " -2.92652e-01  1.40540e-01 -4.57049e-01  1.31094e-01  2.03234e-01\n",
      "  2.94019e-01  7.38370e-02  1.11554e-01 -1.64204e-01 -3.62020e-02\n",
      "  1.29522e-01 -1.28321e-01  1.37502e-01 -7.99200e-03 -5.07100e-03\n",
      " -2.86010e-02 -8.99040e-02  8.82800e-03 -8.27730e-02  6.91940e-02\n",
      " -2.70182e-01  5.47610e-02 -3.06060e-02  6.89880e-02  2.38759e-01\n",
      " -1.41775e-01  2.34763e-01 -2.23853e-01 -2.84994e-01  2.53245e-01\n",
      "  6.77170e-02 -4.39663e-01 -7.00270e-02  6.39150e-02 -9.67100e-02\n",
      " -2.18950e-01 -5.77910e-02 -1.82689e-01 -3.32202e-01 -7.83070e-02\n",
      "  7.74620e-02  8.82920e-02 -4.83618e-01 -1.77812e-01  5.64040e-02\n",
      "  1.50339e-01  8.73000e-02 -1.03121e-01  1.62065e-01  4.57940e-02\n",
      "  9.73590e-02  1.67230e-02  3.00791e-01 -6.49640e-02 -1.95840e-01\n",
      " -4.33790e-02 -9.46810e-02  3.73222e-01 -1.65359e-01  5.58780e-02\n",
      "  1.72660e-02 -3.16048e-01  9.24430e-02 -6.84540e-02 -3.57085e-01\n",
      " -1.69469e-01 -1.14090e-01  9.47230e-02  3.14999e-01  2.12717e-01\n",
      " -2.21540e-02  1.76870e-02  1.58473e-01 -1.39150e-02  1.23610e-02\n",
      " -4.13190e-02 -1.47159e-01 -1.00070e-02  3.41884e-01  1.16999e-01\n",
      " -5.01590e-02  7.88740e-02  6.27940e-02  2.73643e-01  1.46823e-01\n",
      " -1.68857e-01 -1.00014e-01 -5.41060e-02 -3.06130e-02 -8.85920e-02\n",
      " -6.19840e-02  1.21595e-01  1.13775e-01  3.97190e-02 -8.54000e-03\n",
      "  1.05670e-02  1.12375e-01  9.70000e-02  9.05850e-02  1.25026e-01\n",
      " -2.92209e-01  6.81330e-02  4.06070e-02  1.33042e-01 -9.77780e-02\n",
      " -3.26378e-01  9.71420e-02 -5.13600e-02  2.01450e-02  1.20182e-01\n",
      " -2.14210e-02 -1.30884e-01  9.52800e-02 -5.65320e-02 -8.35370e-02\n",
      " -2.53035e-01  9.18650e-02  7.89190e-02 -6.30710e-02 -1.64057e-01\n",
      "  8.31660e-02  1.42698e-01 -2.77053e-01  7.05810e-02 -1.37800e-02\n",
      " -2.74883e-01  3.02011e-01 -8.34330e-02 -1.14381e-01 -2.88826e-01\n",
      "  9.03960e-02  1.94704e-01 -1.57261e-01 -2.58910e-02  1.41321e-01\n",
      " -1.67231e-01 -2.91540e-02  8.03650e-02  1.27378e-01 -1.48120e-01\n",
      "  2.83291e-01 -2.65930e-02  2.15319e-01  3.35030e-02  6.47140e-02\n",
      "  4.20010e-02 -3.85537e-01 -2.67068e-01 -2.77017e-01 -1.82289e-01\n",
      " -1.18735e-01 -2.51480e-01 -1.83783e-01 -2.12362e-01  2.50214e-01\n",
      "  3.96240e-02  2.64830e-02  1.30810e-01 -1.38478e-01 -1.63040e-02\n",
      " -2.55850e-02  2.35141e-01 -8.80540e-02 -9.40650e-02  1.31790e-01\n",
      " -8.33330e-02 -2.40020e-02 -3.38183e-01  8.10370e-02 -1.68933e-01\n",
      "  1.92200e-03  9.34870e-02  6.58130e-02 -1.11925e-01  1.83907e-01\n",
      " -6.54900e-03  4.27730e-02  3.71566e-01 -3.08570e-02  1.99647e-01\n",
      "  1.25516e-01  1.38471e-01 -9.17400e-02 -2.27814e-01  8.27690e-02\n",
      " -2.94581e-01  9.56830e-02 -3.48070e-01  1.02342e-01 -8.05350e-02\n",
      "  2.34290e-02 -4.19860e-02  2.44763e-01  2.37160e-02 -2.23548e-01\n",
      " -9.26800e-03 -4.33650e-02 -1.12413e-01 -4.19178e-01  1.81267e-01\n",
      "  1.03648e-01  2.74945e-01 -9.23560e-02 -4.63300e-02 -2.06314e-01\n",
      "  4.81410e-02  2.64603e-01 -1.17113e-01  1.80097e-01  5.54220e-02\n",
      " -1.27460e-01 -1.60328e-01  1.02289e-01  4.09530e-02  1.25305e-01\n",
      "  1.53398e-01 -2.36950e-02  2.33967e-01  2.30250e-02 -1.40227e-01\n",
      "  3.16349e-01 -1.99592e-01  1.25398e-01  2.72858e-01  1.09793e-01\n",
      " -1.64379e-01  8.63630e-02  1.97445e-01 -2.18180e-02 -1.49784e-01\n",
      " -3.34461e-01 -4.61000e-04  1.92640e-02  2.11149e-01 -2.93349e-01\n",
      "  5.90160e-02  1.25044e-01  7.75570e-02 -2.82863e-01 -3.38890e-02\n",
      " -9.19950e-02 -1.43850e-01  1.45775e-01  1.04246e-01 -2.60548e-01]\n",
      "300\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model.most_similar('china')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('rússia', 0.7320705056190491),\n",
       " ('índia', 0.7241616249084473),\n",
       " ('tailândia', 0.701935887336731),\n",
       " ('indonésia', 0.6860769987106323),\n",
       " ('turquia', 0.6741335988044739),\n",
       " ('malásia', 0.6665689945220947),\n",
       " ('mongólia', 0.6593616008758545),\n",
       " ('manchúria', 0.6581847071647644),\n",
       " ('urss', 0.6581669449806213),\n",
       " ('grã-bretanha', 0.6568097472190857)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model.most_similar('canadá')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('japão', 0.772078812122345),\n",
       " ('quebeque', 0.5880953073501587),\n",
       " ('méxico', 0.5811505913734436),\n",
       " ('chile', 0.5730039477348328),\n",
       " ('brasil', 0.572780430316925),\n",
       " ('havaí', 0.5680618286132812),\n",
       " ('bahrein', 0.5625318288803101),\n",
       " ('haiti', 0.5591698288917542),\n",
       " ('alasca', 0.5584655404090881),\n",
       " ('extremo-oriente', 0.555519700050354)]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model.most_similar('brasil')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('japão', 0.7412912249565125),\n",
       " ('brasil.', 0.6550555229187012),\n",
       " ('pradã£o', 0.6525028944015503),\n",
       " ('haiti', 0.6480737328529358),\n",
       " ('ocidente', 0.6387284994125366),\n",
       " ('trapichão', 0.6379075646400452),\n",
       " ('romildão', 0.6373966336250305),\n",
       " ('emtanto', 0.6351075768470764),\n",
       " ('romildã£o', 0.6259244084358215),\n",
       " ('beira-rio.', 0.6231664419174194)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vetorizacao de texto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "artigo_treino.title.loc[12]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Daniel Craig será stormtrooper em novo 'Star Wars', diz ator\""
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    alphanum_list = []\n",
    "\n",
    "    for valid_token in nltk.word_tokenize(text):\n",
    "        if valid_token in string.punctuation: continue\n",
    "        alphanum_list.append(valid_token)\n",
    "\n",
    "    return alphanum_list"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pedro/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import numpy as np\n",
    "\n",
    "def vector_combination_by_sum(words_numbers):\n",
    "    resulting_vector = np.zeros(300)\n",
    "\n",
    "    for wn in words_numbers:\n",
    "        try:\n",
    "            resulting_vector += model.get_vector(wn)\n",
    "        except KeyError:\n",
    "            if wn.isnumeric():\n",
    "                wn = \"0\"*len(wn)\n",
    "                resulting_vector += model.get_vector(wn)\n",
    "            else:\n",
    "                resulting_vector += model.get_vector(\"unknown\")\n",
    "\n",
    "    \n",
    "    return resulting_vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "words_numbers = tokenizer('texto caelumx')\n",
    "text_vector = vector_combination_by_sum(words_numbers)\n",
    "print(len(text_vector))\n",
    "print(text_vector)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n",
      "[ 4.04902007e-01  1.61033005e-01 -1.28350001e-01  4.63957988e-01\n",
      " -3.79070006e-02  9.79099981e-02  6.06219992e-02  2.50669867e-02\n",
      " -1.45315003e-01 -1.20187001e-01 -1.45029984e-02 -3.97820026e-02\n",
      " -5.53439990e-01  3.39491010e-01  4.74720001e-02  2.32610006e-01\n",
      " -7.91507989e-01  7.62100518e-03 -4.30413000e-01 -1.62210986e-01\n",
      " -1.36736006e-01 -2.92862996e-01 -7.81050026e-02  4.56106991e-01\n",
      " -1.08597010e-01 -1.72903001e-01 -3.94289017e-01 -7.94299692e-03\n",
      "  1.54560007e-01 -1.81829929e-02 -1.44808993e-01 -1.92361996e-01\n",
      "  5.27470000e-02  2.77960012e-02  3.75599414e-03  2.15162002e-01\n",
      "  2.41512008e-01  2.07674988e-01  1.77449007e-01  2.00845003e-01\n",
      "  1.08239995e-02 -1.03557003e-01  4.52470034e-02  3.54460012e-02\n",
      "  2.74897987e-01 -7.71570001e-02  4.26173002e-01  4.57773015e-01\n",
      "  2.98846003e-01  2.85166003e-01  1.75556996e-01  1.93372004e-01\n",
      " -2.64183000e-01  1.87287997e-01  2.00636007e-01 -1.48244001e-01\n",
      "  1.16369996e-01  5.01876995e-01 -2.73249969e-02 -3.36082000e-01\n",
      " -1.06219009e-01  4.23524987e-01  1.28744990e-01 -1.98225003e-01\n",
      "  1.14148993e-01 -1.25489000e-01  4.15736988e-01  2.05809958e-02\n",
      "  2.32367992e-01 -2.06323002e-01  1.77664001e-01 -5.38936019e-01\n",
      "  1.96721995e-01 -1.02734990e-01 -3.09021994e-01  8.97740014e-02\n",
      " -3.16364005e-01 -2.96394996e-01  1.56473998e-01  6.75339978e-02\n",
      " -2.85212003e-01 -4.68930006e-02  5.95229000e-01 -3.89000028e-02\n",
      " -1.57348998e-01 -6.07699156e-03 -4.04890992e-01  2.97449993e-01\n",
      "  1.63218006e-01 -3.78456987e-01 -2.16413990e-01 -4.07410040e-02\n",
      " -9.21399891e-03  3.70410085e-02 -2.91540027e-02  1.31810009e-02\n",
      "  1.34384997e-01  1.30410001e-01  9.68170017e-02 -3.55980992e-01\n",
      "  2.14397006e-01 -2.98079006e-01  2.54114997e-01 -1.51478000e-01\n",
      "  1.95116995e-01  2.09010001e-01 -1.63654998e-01  3.20399009e-01\n",
      " -2.51417004e-01  2.53486998e-01  4.42780033e-02  2.24656996e-01\n",
      "  8.25779997e-02 -2.04713996e-01 -3.06100026e-03  2.70969868e-02\n",
      " -1.38156980e-01 -2.51079008e-01  1.61626004e-01  1.95170045e-02\n",
      "  3.43521997e-01 -3.87043998e-01 -8.47560018e-02 -3.38781985e-01\n",
      " -1.14501003e-01 -7.03117013e-01 -2.56285995e-01  4.24469993e-02\n",
      " -1.62145995e-01  4.75210987e-01 -2.77539000e-01  7.60199875e-02\n",
      " -1.06837997e-01 -9.87000763e-04  1.72742993e-01 -2.99829011e-01\n",
      "  2.28087999e-01  3.52940038e-02  2.89734993e-01  2.79758990e-01\n",
      " -2.87392993e-01  8.17699730e-03  2.14280997e-01  2.25391012e-01\n",
      " -7.59299994e-02 -1.23466007e-01 -2.72139981e-02 -2.64430046e-02\n",
      "  5.86279005e-01 -2.40319993e-02 -2.68981993e-01  2.25396007e-01\n",
      " -3.03109884e-02 -2.71128006e-01 -1.62265003e-01  6.46600127e-03\n",
      "  3.39117015e-01  5.28680012e-02  1.92705989e-01 -5.51650003e-02\n",
      "  1.44278001e-01  2.49232002e-01 -5.09869996e-02 -3.06719005e-01\n",
      " -2.48060040e-02  2.31652994e-01  2.77169995e-01  1.89650998e-01\n",
      " -6.40159994e-02  5.75810010e-02 -9.78650004e-02  4.85200435e-03\n",
      "  4.45419997e-02 -1.88349998e-01 -6.07778013e-01 -1.39748003e-01\n",
      "  3.68884996e-01 -3.19058999e-01 -7.32080005e-02 -4.49153997e-01\n",
      " -2.37713000e-01 -1.51614994e-01 -3.09304014e-01 -1.56432007e-01\n",
      "  4.18815009e-01 -3.89327988e-01  3.78332011e-01 -4.00199965e-02\n",
      " -6.51219994e-01 -5.50040156e-02 -1.93998218e-04 -9.10810027e-02\n",
      " -2.88987003e-01  1.36337005e-01  1.07480004e-01  5.68140998e-01\n",
      " -3.27486984e-01 -4.62799985e-01  2.03891002e-01  1.49019003e-01\n",
      " -2.44706005e-01 -1.99469005e-01  8.01479965e-02  2.98199998e-02\n",
      " -1.31894002e-01  1.15945004e-01  8.99800286e-03 -3.35849009e-01\n",
      "  3.68386969e-01  3.43659982e-01  4.19899821e-03  8.20260011e-02\n",
      "  4.53375012e-01  6.57530016e-02  7.55139990e-02  3.60649996e-02\n",
      "  5.19831993e-01 -1.47042006e-01 -2.90375011e-01 -1.32993003e-01\n",
      " -2.00269002e-01  1.88818000e-01 -2.63435002e-01 -1.86488993e-01\n",
      "  1.89731000e-01  1.17092013e-01 -6.79409951e-02 -7.07589984e-02\n",
      " -2.77480036e-02 -1.21008001e-01 -2.21680000e-01 -2.02926997e-01\n",
      "  1.03461001e-01 -4.55000028e-02 -1.61133002e-01 -2.65853986e-01\n",
      " -3.72106001e-01  1.30873001e-01 -1.40520036e-02 -3.46357003e-01\n",
      " -2.55110003e-02  3.54763001e-01  1.88224000e-01 -6.63699955e-03\n",
      "  6.33194998e-01  2.15856001e-01  1.53947005e-01  4.10899967e-02\n",
      "  3.06820998e-01  1.06974006e-01 -7.21679926e-02  1.75355002e-01\n",
      " -5.81880002e-02 -2.46847994e-01  2.00998999e-01  3.76706995e-01\n",
      "  3.04477009e-01 -5.40189967e-02  3.21371995e-01 -2.00102992e-01\n",
      "  1.52778007e-01  1.51184998e-01  1.51455004e-01  1.15802005e-01\n",
      "  2.59180069e-02 -1.06971003e-01  3.22931997e-01 -3.25010000e-02\n",
      "  3.03647995e-01 -2.75807986e-01 -2.92581989e-01  4.18120027e-02\n",
      "  2.87143990e-01  4.63455006e-01 -2.67790999e-01  6.72599673e-03\n",
      "  2.14253999e-01  2.98653997e-01  2.97100022e-02 -4.63071994e-01\n",
      " -2.20077008e-01 -2.79074993e-01 -1.11349970e-02  1.69707993e-01\n",
      "  2.01697998e-01 -2.11112998e-01 -5.53999841e-03 -3.71473014e-01\n",
      " -6.25945002e-01 -4.84354988e-01  5.33823982e-01 -8.99890019e-02\n",
      " -1.57629006e-01  1.03843000e-01 -1.16449997e-01  4.12781991e-01\n",
      "  4.35150117e-02 -5.21820009e-01  6.19100034e-02  8.41154024e-01]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def vector_matrix(texts):\n",
    "    x = len(texts)\n",
    "    y = 300\n",
    "\n",
    "    matrix = np.zeros((x, y))\n",
    "\n",
    "    for i in range(x):\n",
    "        words_numbers = tokenizer(texts.iloc[i])\n",
    "        matrix[i] = vector_combination_by_sum(words_numbers)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "vector_matrix_train = vector_matrix(artigo_treino['title'])\n",
    "vector_matrix_test = vector_matrix(artigo_teste['title'])\n",
    "\n",
    "\n",
    "print(vector_matrix_train.shape)\n",
    "print(vector_matrix_test.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(90000, 300)\n",
      "(20513, 300)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticRegressionModel = LogisticRegression(max_iter = 200)\n",
    "logisticRegressionModel.fit(vector_matrix_train, artigo_treino['category'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "logisticRegressionModel.score(vector_matrix_test, artigo_teste['category'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7957880368546775"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "artigo_teste['category'].unique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['colunas', 'esporte', 'mercado', 'cotidiano', 'mundo', 'ilustrada'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted_label = logisticRegressionModel.predict(vector_matrix_test)\n",
    "\n",
    "CR = classification_report(artigo_teste['category'], predicted_label)\n",
    "print(CR)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.86      0.71      0.78      6103\n",
      "   cotidiano       0.61      0.79      0.69      1698\n",
      "     esporte       0.92      0.88      0.90      4663\n",
      "   ilustrada       0.13      0.88      0.23       131\n",
      "     mercado       0.84      0.79      0.81      5867\n",
      "       mundo       0.74      0.86      0.79      2051\n",
      "\n",
      "    accuracy                           0.80     20513\n",
      "   macro avg       0.68      0.82      0.70     20513\n",
      "weighted avg       0.83      0.80      0.81     20513\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "DC = DummyClassifier()\n",
    "DC.fit(vector_matrix_train, artigo_treino['category'])\n",
    "predicted_label_dc = DC.predict(vector_matrix_test)\n",
    "\n",
    "CR_dummy = classification_report(artigo_teste['category'], predicted_label_dc)\n",
    "print(CR_dummy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.30      1.00      0.46      6103\n",
      "   cotidiano       0.00      0.00      0.00      1698\n",
      "     esporte       0.00      0.00      0.00      4663\n",
      "   ilustrada       0.00      0.00      0.00       131\n",
      "     mercado       0.00      0.00      0.00      5867\n",
      "       mundo       0.00      0.00      0.00      2051\n",
      "\n",
      "    accuracy                           0.30     20513\n",
      "   macro avg       0.05      0.17      0.08     20513\n",
      "weighted avg       0.09      0.30      0.14     20513\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/pedro/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pedro/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pedro/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# http://www.nilc.icmc.usp.br/embeddings\n",
    "model_skipgram = KeyedVectors.load_word2vec_format('skip_s300.txt')\n",
    "\n",
    "def vector_combination_by_sum_skipgram(words_numbers):\n",
    "    resulting_vector = np.zeros(300)\n",
    "\n",
    "    for wn in words_numbers:\n",
    "        try:\n",
    "            resulting_vector += model_skipgram.get_vector(wn)\n",
    "        except KeyError:\n",
    "            if wn.isnumeric():\n",
    "                wn = \"0\"*len(wn)\n",
    "                resulting_vector += model_skipgram.get_vector(wn)\n",
    "            else:\n",
    "                resulting_vector += model_skipgram.get_vector(\"unknown\")\n",
    "\n",
    "    \n",
    "    return resulting_vector\n",
    "\n",
    "def vector_matrix_skipgram(texts):\n",
    "    x = len(texts)\n",
    "    y = 300\n",
    "\n",
    "    matrix = np.zeros((x, y))\n",
    "\n",
    "    for i in range(x):\n",
    "        words_numbers = tokenizer(texts.iloc[i])\n",
    "        matrix[i] = vector_combination_by_sum_skipgram(words_numbers)\n",
    "\n",
    "    return matrix\n",
    "   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "vector_matrix_train_skipgram = vector_matrix_skipgram(artigo_treino['title'])\n",
    "vector_matrix_test_skipgram = vector_matrix_skipgram(artigo_teste['title'])\n",
    "\n",
    "#logistic regression\n",
    "LR_skipgram = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "LR_skipgram.fit(vector_matrix_train_skipgram, artigo_treino['category'])\n",
    "label_prediction_skipgram = LR_skipgram.predict(vector_matrix_test_skipgram)\n",
    "\n",
    "CR_skipgram = classification_report(artigo_teste['category'], label_prediction_skipgram)\n",
    "print(CR_skipgram)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.86      0.72      0.78      6103\n",
      "   cotidiano       0.63      0.81      0.70      1698\n",
      "     esporte       0.93      0.89      0.91      4663\n",
      "   ilustrada       0.15      0.91      0.26       131\n",
      "     mercado       0.84      0.81      0.83      5867\n",
      "       mundo       0.76      0.86      0.80      2051\n",
      "\n",
      "    accuracy                           0.81     20513\n",
      "   macro avg       0.69      0.83      0.71     20513\n",
      "weighted avg       0.84      0.81      0.82     20513\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "print(CR)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.86      0.71      0.78      6103\n",
      "   cotidiano       0.61      0.79      0.69      1698\n",
      "     esporte       0.92      0.88      0.90      4663\n",
      "   ilustrada       0.13      0.88      0.23       131\n",
      "     mercado       0.84      0.79      0.81      5867\n",
      "       mundo       0.74      0.86      0.79      2051\n",
      "\n",
      "    accuracy                           0.80     20513\n",
      "   macro avg       0.68      0.82      0.70     20513\n",
      "weighted avg       0.83      0.80      0.81     20513\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec52e35cbadde383e76887bf156a3d142d1ed012c2f29b766319110f28479c8f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}